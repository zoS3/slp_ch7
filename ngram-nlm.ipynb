{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "n-gramのneural language modelで学習と予測を行うプログラム\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class NLM(nn.Module):\n",
    "    \"\"\"\n",
    "    n-gram neural language model\n",
    "    \"\"\"\n",
    "    def __init__(self, context_size, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear_W = nn.Linear(context_size*embedding_dim, hidden_dim)\n",
    "        self.linear_U = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, context_word_ids):\n",
    "        # shape: (1, context_size*embedding_dim)\n",
    "        projection_layer = self.embedding(context_word_ids).view(1, -1)\n",
    "        # shape: (1, hidden_dim)\n",
    "        hidden_layer = torch.relu(self.linear_W(projection_layer))\n",
    "        # shape: (1, vocab_size)\n",
    "        output_layer = self.linear_U(hidden_layer)\n",
    "        # shape: (1, vocab_size)\n",
    "        return torch.log_softmax(output_layer, dim=1)\n",
    "\n",
    "    \n",
    "def build_vocab(sentences):\n",
    "    all_tokens = []\n",
    "    for s in sentences:\n",
    "        all_tokens += s.split()\n",
    "    vocab = set(all_tokens)\n",
    "    token_to_index = dict(zip(vocab, range(len(vocab))))\n",
    "    index_to_token = {index:token for token, index in token_to_index.items()}\n",
    "    return token_to_index, index_to_token\n",
    "\n",
    "\n",
    "def ngram_generator(context_size, sentences, token_to_index):\n",
    "    \"\"\"\n",
    "    yield (n-gram context token ids tensor, target token id tensor)\n",
    "    e.g. (tensor([12,  0, 10]), tensor([4]))\n",
    "    corresponding to (['I', 'have', 'to'], 'make')\n",
    "    \"\"\"\n",
    "    for s in sentences:\n",
    "        tokens = s.split()\n",
    "        for t in range(len(tokens)):\n",
    "            if t+context_size == len(tokens):\n",
    "                break\n",
    "            context, target = tokens[t:t+context_size], tokens[t+context_size]\n",
    "            context = torch.tensor(\n",
    "                [token_to_index[w] for w in context], dtype=torch.long)\n",
    "            target = torch.tensor(\n",
    "                [token_to_index[target]], dtype=torch.long)\n",
    "            yield context, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I have to make sure when I get home to foo the cat .\",\n",
    "             \"I have to make foo when I get home to feed the cat .\",\n",
    "             \"I have to make sure when you get home to feed the cat .\",\n",
    "             \"I have to make sure when I get house to feed the dog .\"]\n",
    "token_to_index, index_to_token = build_vocab(sentences)\n",
    "context_size = 3  # ngram size - 1\n",
    "model = NLM(context_size=context_size, vocab_size=len(token_to_index), embedding_dim=32, hidden_dim=32)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# training\n",
    "for _ in range(30):\n",
    "    for context, target in ngram_generator(context_size, sentences, token_to_index):\n",
    "        optimizer.zero_grad()\n",
    "        log_prob = model(context)\n",
    "        loss = criterion(log_prob, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus sentences:\n",
      "I have to make sure when I get home to foo the cat .\n",
      "I have to make foo when I get home to feed the cat .\n",
      "I have to make sure when you get home to feed the cat .\n",
      "I have to make sure when I get house to feed the dog .\n",
      "\n",
      "predicted sentence:\n",
      "I have to make sure when I get home to feed the cat . "
     ]
    }
   ],
   "source": [
    "print('corpus sentences:')\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "print()\n",
    "\n",
    "context_words = ['I', 'have', 'to']\n",
    "assert len(context_words) == context_size\n",
    "print('predicted sentence:')\n",
    "for w in context_words:\n",
    "    print(w, end=' ')\n",
    "\n",
    "# predict\n",
    "while True:\n",
    "    context_tensor = torch.tensor([token_to_index[w] for w in context_words])\n",
    "    predicted_log_prob = model(context_tensor)\n",
    "    next_word_index = torch.argmax(predicted_log_prob).item()\n",
    "    next_word = index_to_token[next_word_index]\n",
    "    print(next_word, end=' ')\n",
    "    if next_word == '.':\n",
    "        break\n",
    "    context_words.pop(0)\n",
    "    context_words.append(next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コーパスにないが最も妥当な文を生成できている\n",
    "# context_sizeを変えても面白いかもしれない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
